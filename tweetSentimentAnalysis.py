# -*- coding: utf-8 -*-
"""tweet_sentiment_analysis_VADER.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D457lV8CIdGRqdJsiSOBsHhrvFxGoerD
"""

# Based on: https://github.com/nicknochnack/TwitterPresidentialDebate/blob/main/Debate%20Analysis.ipynb
# and https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/
# Using VADER for sentiment analysis
# https://github.com/cjhutto/vaderSentiment


# Import tweepy to work with the twitter API
import tweepy as tweepy
# Import pandas to work with dataframes
import pandas as pd
# !pip install vaderSentiment
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
# Import Regex
import re

# Use environment variables
import os

# Import twitter credential keys
import keys

def authTwitter(twitterCredentials:dict):
  '''
  Authenticate tweepy api based on credentials

          Parameters:
                  twitterCredentials (dict): A dictionary containing credentials needed to access tweepy. Works with twitter api v1

          Returns:
                  (API): Tweepy Twitter API v1.1 Interface
  '''
  # Authenticate
  auth = tweepy.OAuth1UserHandler(twitterCredentials['API_KEY'], twitterCredentials['API_SECRET'])
  # Set Tokens
  auth.set_access_token(twitterCredentials['ACCESS_TOKEN'], twitterCredentials['ACCESS_TOKEN_SECRET'])
  # Instantiate API
  api = tweepy.API(auth, wait_on_rate_limit=True)
  return api

def retrieveTweets(searchQuery:str, numItems:int, twitterCredentials:dict):
  '''
  Retrieve tweets based on search query.

  - Note: Search index has a 7-day limit. No tweets will be found for a date older than one week.


          Parameters:
                  searchQuery (str): String containing the search query used to reteieve tweets. 
                  Can use modifiers such as 'from:[username]', '#[term]', '@[username]' etc.

                  numItems(int): Number of tweets to be returned

                  twitterCredentials (dict): A dictionary containing credentials needed to access tweepy. Works with twitter api v1

          Returns:
                  (dict): list of dicts for each tweet
  '''
  print(f'Retrieving {numItems} tweets based on: "{searchQuery}"... \n')
  api = authTwitter(twitterCredentials)
  # Retieve tweets that are in eng and that are NOT rts
  query = tweepy.Cursor(api.search_tweets, q=f'{searchQuery} -filter:retweets', lang='en', tweet_mode='extended').items(numItems)
  baseUrl = 'https://twitter.com/i/web/status/'
  tweets = [{'tweet':tweet.full_text, 'timestamp':str(tweet.created_at), 'url':f'{baseUrl}{tweet.id}', 'tweet_id':str(tweet.id)} for tweet in query]
  return tweets

def preprocessTweet(tweet:str):
    '''
    Cleans up tweet by removing stopwords, urls, and replacing unnecessary chars 

            Parameters:
                    tweet (str): A string containing a single tweet to be preprocessed

            Returns:
                    (str): A string that has been processed and cleaned up
    '''

    custom_stopwords = set(['rt', 'u', 'lol', '(', ')', '()', '?'])

    # Stopwords from https://www.ranks.nl/stopwords
    stopwords = set(line.strip().lower() for line in open('wordlists/stopwords.txt'))

    # Converting text to lowercase and replacing the apostrophe
    processed_tweet = tweet.lower().replace("’", "'")

    # Removing @ and username
    processed_tweet = re.sub(r'@[/^\w+$/]+:', '', processed_tweet)
    # Removing @ and username
    processed_tweet = re.sub("[@]\w+", '', processed_tweet)
    # Removing URLs
    processed_tweet = re.sub(r'http://\S+|https://\S+', '', processed_tweet)

    # Removing custom stopwords
    processed_tweet = " ".join(word for word in processed_tweet.split() if word not in custom_stopwords).strip()

    # Removing stopwords
    processed_tweet = " ".join(word for word in processed_tweet.split() if word not in stopwords).strip()

    return(processed_tweet)

def preprocessTweets(df:pd.DataFrame):
      '''
      Creates a new column containing processed tweets

              Parameters:
                      df (DataFrame): A dataframe containing tweets to be processed

              Returns:
                      (DataFrame): A dataframe with a column containing processed tweets
      '''
      df['processed_tweet'] = df['tweet'].apply(lambda x: preprocessTweet(x))
      return df


def createWordcountDf(df:pd.DataFrame, numWords:int, searchQuery:str):
  '''
  Create a new df containing the most common words found in the tweets and their number of occurrances

          Parameters:
                  df (DataFrame): A dataframe containing processed tweets

                  numWords (int): An interger containing the amount of words to be returned

                  searchQuery (str): A string containing the search term used to retrive tweets

          Returns:
                  (DataFrame): A dataframe containing a column for the top common words found in the tweets and a column for their number of occurrances
  '''
  # Instantiate a dictionary, and for every word in the tweet, 
  # Add to the dictionary if it doesn't exist. If it does, increase the count.
  wordcount = {}
  # Longer list of stopwords from https://algs4.cs.princeton.edu/35applications/stopwords.txt
  more_stopwords = set(line.strip().lower() for line in open('wordlists/more_stopwords.txt'))

  sentences = [w.split() for w in df['processed_tweet'].tolist()]
  for sentence in sentences:
    for word in sentence:

      # If the word is in the more stopwords list, do not add to wordcount
      if word in more_stopwords:
        continue

      # Removing non-alphaneumeric symbols
      word = word.lower()
      word = word.replace(".","")
      word = word.replace(",","")
      word = word.replace(":","")
      word = word.replace('"',"")
      word = word.replace("!","")
      word = word.replace("?","")
      word = word.replace("â€œ","")
      word = word.replace("â€˜","")
      word = word.replace("*","")
      word = word.replace('&amp;', '')
      word = word.replace('#', '')
      word = word.replace('-', '')
      word = word.replace('(', '')
      word = word.replace(')', '')

      # If the word is an empty string, do not add to wordcount
      if not word:
        continue
      
      # If the word is in the search query, do not add to wordcount 
      if word in searchQuery:
        continue

      if word not in wordcount:
          wordcount[word] = 1
      else:
          wordcount[word] += 1


  wordcount = {'word': list(wordcount.keys()), 'count': list(wordcount.values())}
  wordcount_df = pd.DataFrame.from_dict(wordcount)

  # Only include words with more than one occurrance and filter in decending order by occurrances
  wordcount_df = wordcount_df[wordcount_df['count'] > 1].sort_values(by=['count'], ascending=False)
  return wordcount_df.head(numWords)

def isPos(x:float):
    '''
    Determine wether a tweet is positive (eg. polarity > 0)

            Parameters:
                    x (float): A polarity score

            Returns:
                    (int): Returns 1 if it is pos, 0 if not
    '''
    if x > 0:
      return 1
    else:
      return 0

def isNeu(x:float):
    '''
    Determine wether a tweet is neutral (eg. polarity == 0)

            Parameters:
                    x (float): A polarity score

            Returns:
                    (int): Returns 1 if it is neu, 0 if not
    '''
    if x == 0:
      return 1
    else:
      return 0

def isNeg(x:float):
    '''
    Determine wether a tweet is negative (eg. polarity < 0)

            Parameters:
                    x (float): A polarity score

            Returns:
                    (int): Returns 1 if it is neg, 0 if not
    '''
    if x < 0:
      return 1
    else:
      return 0

def calculatePolarity(df:pd.DataFrame):
  '''
  Adding columns to df containing a polarity score and a pos, neg, or neu value

          Parameters:
                  df (DataFrame): A dataframe containing processed tweets

          Returns:
                  (DataFrame): A dataframe with columns for a polarity score and a pos, neg, or neu value
  '''
  print('Calculating polarity of tweets... \n')
  # Calculate polarity
  analyzer = SentimentIntensityAnalyzer()
  df['polarity'] = df['processed_tweet'].apply(lambda x: analyzer.polarity_scores(x)['compound'])
        
  # Creating columns identifying whether a tweet is positive, negative, and neutral
  df['pos'] = df['polarity'].apply(lambda x: isPos(x))
  df['neu'] = df['polarity'].apply(lambda x: isNeu(x))
  df['neg'] = df['polarity'].apply(lambda x: isNeg(x))

  return df

def getTweetData(term:str, numItems:int, numWordCount:int):
        '''
        Main function for generating tweet sentiment analysis data

                Parameters:
                        term (str): String containing the search query used to reteieve tweets. 
                        Can use modifiers such as 'from:[username]', '#[term]', '@[username]' etc.

                        numItems(int): Number of tweets to be returned

                        numWordCount(int): Number of word count words to be returned

                Returns:
                        (dict): A dict containing polarity data of tweets, wordcount data of tweets, and csv file strings of data
        '''

        # Constants
        TWITTER_CREDENTIALS = {
        'API_KEY' : os.environ['API_KEY'],
        'API_SECRET' : os.environ['API_SECRET'],
        'ACCESS_TOKEN' : os.environ['ACCESS_TOKEN'],
        'ACCESS_TOKEN_SECRET' : os.environ['ACCESS_TOKEN_SECRET']
        }

        # Retrieving tweets and returning results as a dict
        tweets = retrieveTweets(term, numItems, TWITTER_CREDENTIALS)

        # Converting results to dataframe
        df = pd.DataFrame.from_dict(tweets)

        # Preprocessing tweets
        df = preprocessTweets(df)

        # Creating df of most common words and their occurrances
        wordcount_df = createWordcountDf(df, numWordCount, term)

        # Creating df of tweets and their polarity
        sentiment_df = calculatePolarity(df)

        # Converting to dict
        sentiment_dict = sentiment_df.to_dict(orient="records")
        wordcount_dict = wordcount_df.to_dict(orient="records")

        # Combining data to a single dict
        results = {
                'sentiment': sentiment_dict,
                'wordcount': wordcount_dict
        }

        # Returns a dict containing data
        return results
